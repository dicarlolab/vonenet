{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VOneResNet18-distillation.ipynb","provenance":[],"collapsed_sections":["V9g6T01pPuHi","raBXY53EP13q","vjKu1JvxP8El","zHiIrAr1vmPj","iUmmQRReRMIi"],"mount_file_id":"1Qdcb0Qnm4qte9pf0PO7c7fuW9OJMTpSy","authorship_tag":"ABX9TyNEUDJxtPAkPsydUDT3nahm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nU8FLPmkTMRn","executionInfo":{"status":"ok","timestamp":1630675589550,"user_tz":240,"elapsed":19793,"user":{"displayName":"Avinash","photoUrl":"","userId":"03822310646586674578"}},"outputId":"57132d04-d52f-45e3-bd30-41e4b789abd0"},"source":["!git clone https://github.com/baiydaavi/vonenet.git\n","%cd vonenet/\n","!git checkout avi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'vonenet'...\n","remote: Enumerating objects: 184, done.\u001b[K\n","remote: Counting objects: 100% (24/24), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 184 (delta 13), reused 21 (delta 10), pack-reused 160\u001b[K\n","Receiving objects: 100% (184/184), 508.06 MiB | 35.32 MiB/s, done.\n","Resolving deltas: 100% (87/87), done.\n","Checking out files: 100% (27/27), done.\n","/content/vonenet\n","Already on 'avi'\n","Your branch is up to date with 'origin/avi'.\n"]}]},{"cell_type":"code","metadata":{"id":"8H01w2VlUJD_"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","import torchvision.datasets as datasets\n","import matplotlib.pyplot as plt\n","import os, glob\n","from PIL import Image\n","from google.colab import files\n","\n","from vonenet import ResNet18"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V9g6T01pPuHi"},"source":["#login to weights and biases to monitor training"]},{"cell_type":"code","metadata":{"id":"WM6SJbi2UMjM"},"source":["%%capture\n","!pip install wandb --upgrade\n","import wandb\n","wandb.login()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"raBXY53EP13q"},"source":["#Download Tiny ImageNet data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wOUBSbyOUZSp","executionInfo":{"status":"ok","timestamp":1630675742792,"user_tz":240,"elapsed":25590,"user":{"displayName":"Avinash","photoUrl":"","userId":"03822310646586674578"}},"outputId":"0cec4d0b-fe01-4105-ebce-f0ed36fe858b"},"source":["if not os.path.exists('/content/tiny-imagenet-200/'):\n","    !wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n","    !unzip -q tiny-imagenet-200.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-09-03 13:28:37--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n","Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n","Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 248100043 (237M) [application/zip]\n","Saving to: ‘tiny-imagenet-200.zip’\n","\n","tiny-imagenet-200.z 100%[===================>] 236.61M  11.2MB/s    in 18s     \n","\n","2021-09-03 13:28:54 (13.4 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"vjKu1JvxP8El"},"source":["#Define training and testing dataset classes"]},{"cell_type":"code","metadata":{"id":"HbQ0V_XLUfNd"},"source":["class TrainTinyImageNetDataset(Dataset):\n","    def __init__(self, id, transform=None):\n","        self.filenames = glob.glob(\"tiny-imagenet-200/train/*/*/*.JPEG\")\n","        self.transform = transform\n","        self.id_dict = id\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.filenames[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        label = self.id_dict[img_path.split('/')[2]]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","class TestTinyImageNetDataset(Dataset):\n","    def __init__(self, id, transform=None):\n","        self.filenames = glob.glob(\"tiny-imagenet-200/val/images/*.JPEG\")\n","        self.transform = transform\n","        self.id_dict = id\n","        self.cls_dic = {}\n","        for i, line in enumerate(open('tiny-imagenet-200/val/val_annotations.txt', 'r')):\n","            a = line.split('\\t')\n","            img, cls_id = a[0],a[1]\n","            self.cls_dic[img] = self.id_dict[cls_id]\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.filenames[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        label = self.cls_dic[img_path.split('/')[-1]]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kih3wBrHUg_2"},"source":["id_dict = {}\n","for i, line in enumerate(open('tiny-imagenet-200/wnids.txt', 'r')):\n","  id_dict[line.replace('\\n', '')] = i\n","\n","transform_train = transforms.Compose([\n","        transforms.RandomAffine(degrees=30, translate=(0.05, 0.05), scale=(1.0, 1.2)),\n","        transforms.RandomHorizontalFlip(0.5),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","\n","transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zHiIrAr1vmPj"},"source":["#Ensemble Model\n"]},{"cell_type":"code","metadata":{"id":"qLYAZde55Xtz"},"source":["class EnsembleModel(nn.Module):\n","\n","    def __init__(self, model_dict, num_classes = 200):\n","        super(EnsembleModel, self).__init__()\n","\n","        self.model_dict = model_dict\n","        self.models = nn.ModuleList()\n","\n","        for i, model_type in enumerate(self.model_dict):\n","            model_params = self.model_dict[model_type]['model_params']\n","            model_path = self.model_dict[model_type]['model_path']\n","            \n","            model = VOneNet(\n","                model_arch='resnet18', \n","                noise_mode=model_params['noise_mode'], \n","                noise_scale=0.286, \n","                poisson_scale=model_params['poisson_scale'], \n","                noise_level=0.071, \n","                image_size=64, \n","                visual_degrees=2, \n","                sf_max=model_params['sf_max'], \n","                sf_min=model_params['sf_min'],\n","                simple_channels=model_params['simple_channels'],\n","                complex_channels=model_params['complex_channels'],\n","                stride=2, \n","                ksize=25, \n","                k_exc=23.5\n","                )\n","            \n","            model.load_state_dict(torch.load(model_path)['net'])\n","\n","            self.models.append(model)\n","\n","    def forward(self, x):\n","        \n","        out = self.models[0](x)\n","        for i in range(1, len(self.model_dict)):\n","            out += self.models[i](x)\n","\n","        return out "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iketNXb0Dljb"},"source":["# Defining the variants use to create the Ensemble model\n","param_dict = {}\n","param_dict['sf_low'] = {'noise_mode':'neuronal', 'poisson_scale':1.0, 'sf_max':2.0, \n","                        'sf_min':0, 'simple_channels':256, 'complex_channels':256}\n","param_dict['sf_mid'] = {'noise_mode':'neuronal', 'poisson_scale':1.0, 'sf_max':5.6,\n","                        'sf_min':2.0, 'simple_channels':256, 'complex_channels':256}\n","param_dict['sf_high'] = {'noise_mode':'neuronal', 'poisson_scale':1.0, 'sf_max':11.3,\n","                         'sf_min':5.6, 'simple_channels':256, 'complex_channels':256}\n","param_dict['no_noise'] = {'noise_mode':None, 'poisson_scale':0.0, 'sf_max':11.3, \n","                        'sf_min':0.0, 'simple_channels':256, 'complex_channels':256}\n","param_dict['low_noise'] = {'noise_mode':'neuronal', 'poisson_scale':0.5, 'sf_max':11.3, \n","                        'sf_min':0.0, 'simple_channels':256, 'complex_channels':256}\n","param_dict['normal_noise'] = {'noise_mode':'neuronal', 'poisson_scale':1.0, 'sf_max':11.3, \n","                        'sf_min':0.0, 'simple_channels':256, 'complex_channels':256}\n","param_dict['only_simple'] = {'noise_mode':'neuronal', 'poisson_scale':1.0, 'sf_max':11.3, \n","                        'sf_min':0.0, 'simple_channels':512, 'complex_channels':0}\n","param_dict['only_complex'] = {'noise_mode':'neuronal', 'poisson_scale':1.0, 'sf_max':11.3,\n","                        'sf_min':0.0, 'simple_channels':0, 'complex_channels':512}\n","\n","model_dict = {}\n","for model_type in param_dict:\n","    model_dict[model_type] = {'model_params':param_dict[model_type],\n","                              'model_path':f'/content/drive/MyDrive/github/vonenet/checkpoint/best_models/{model_type}.pth'}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUmmQRReRMIi"},"source":["#Train model"]},{"cell_type":"code","metadata":{"id":"aM55dXL7UtMz"},"source":["class TrainModel:\n","    def __init__(self, model_dict, batch_size=128, save_dir=None):\n","        \n","        self.EnsembleModel = EnsembleModel(model_dict)\n","\n","        self.DistilledModel = VOneNet(model_arch='resnet18',\n","                                    noise_mode=None,\n","                                    noise_scale=0.286, \n","                                    poisson_scale = 0.0, \n","                                    noise_level=0.071, \n","                                    image_size=64, \n","                                    visual_degrees=2, \n","                                    sf_max=11.3, \n","                                    stride=2, \n","                                    ksize=25, \n","                                    k_exc=23.5)\n","        \n","        self.kl_div_loss = nn.KLDivLoss(log_target=True)\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.temperature = 5.\n","        self.soft_targets_weight = 100.\n","        self.label_loss_weight = 0.5\n","        self.optimizer = optim.SGD(self.DistilledModel.parameters(), lr=0.1,\n","                            momentum=0.9, weight_decay=5e-4)\n","        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', patience = 5)\n","        \n","        self.trainset = TrainTinyImageNetDataset(id=id_dict, transform=transform_train)\n","        self.testset = TestTinyImageNetDataset(id=id_dict, transform=transform_test)\n","        self.trainloader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=True)\n","        self.testloader = torch.utils.data.DataLoader(self.testset, batch_size=batch_size, shuffle=False)\n","\n","        self.train_loss_vec = []\n","        self.train_acc_vec = []\n","        self.test_loss_vec = []\n","        self.test_acc_vec = []\n","        self.best_acc = 0  # best test accuracy\n","\n","        self.save_dir = save_dir\n","        if not os.path.isdir(self.save_dir):\n","                os.mkdir(self.save_dir)\n","\n","    def train(self, epoch):\n","\n","        # training\n","        print('\\nEpoch: %d' % epoch)\n","        self.EnsembleModel.eval()\n","        self.DistilledModel.train()\n","        train_loss = 0\n","        correct = 0\n","        total = 0\n","        for batch_idx, (inputs, targets) in enumerate(self.trainloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            with torch.no_grad():\n","                ensemble_logits = self.EnsembleModel(inputs)\n","            self.optimizer.zero_grad()\n","            outputs = self.DistilledModel(inputs)\n","            soft_targets = nn.functional.log_softmax(ensemble_logits / self.temperature, dim=-1)\n","            soft_prob = nn.functional.log_softmax(outputs/self.temperature, dim=-1)\n","            soft_targets_loss = self.kl_div_loss(soft_prob, soft_targets)\n","            label_loss = self.criterion(outputs, targets) #.to(device)\n","            loss = self.soft_targets_weight * soft_targets_loss + self.label_loss_weight * label_loss\n","            loss.backward()\n","            self.optimizer.step()\n","            \n","            train_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","        \n","        training_loss = train_loss/(batch_idx+1)\n","        training_acc = 100.*correct/total\n","\n","        print(f'Training Loss: {training_loss} | Training Acc: {training_acc} ({correct}/{total})')\n","        self.train_loss_vec.append(training_loss)\n","        self.train_acc_vec.append(training_acc)\n","\n","        # validation\n","        self.DistilledModel.eval()\n","        test_loss = 0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for batch_idx, (inputs, targets) in enumerate(self.testloader):\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                ensemble_logits = self.EnsembleModel(inputs)\n","                outputs = self.DistilledModel(inputs)\n","                soft_targets = nn.functional.log_softmax(ensemble_logits / self.temperature, dim=-1)\n","                soft_prob = nn.functional.log_softmax(outputs/self.temperature, dim=-1)\n","                soft_targets_loss = self.kl_div_loss(soft_prob, soft_targets)\n","                label_loss = self.criterion(outputs, targets) #.to(device)\n","                loss = self.soft_targets_weight * soft_targets_loss + self.label_loss_weight * label_loss\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","            \n","        self.scheduler.step(test_loss)\n","\n","        validation_loss = test_loss/(batch_idx+1)\n","        validation_acc = 100.*correct/total\n","\n","        print(f'Validation Loss: {validation_loss} | Validation Acc: {validation_acc} ({correct}/{total})')\n","\n","        self.test_loss_vec.append(validation_loss)\n","        self.test_acc_vec.append(validation_acc)\n","\n","        # logging to wandb\n","        wandb.log({\"Epoch\": epoch,        \n","           \"Train Loss\": training_loss,        \n","           \"Train Acc\": training_acc,        \n","           \"Valid Loss\": validation_loss,        \n","           \"Valid Acc\": validation_acc})\n","\n","        # Save checkpoint.\n","        if validation_acc > self.best_acc:\n","            print('Saving..')\n","            state = {\n","                'net': self.DistilledModel.state_dict(),\n","                'acc': validation_acc,\n","                'epoch': epoch,\n","            }\n","            torch.save(state, f'{self.save_dir}/{epoch}.pth')\n","            self.best_acc = validation_acc\n","\n","    def check_accuracy(self, pretrained_path = None):    \n","        if pretrained_path:\n","            print('loading pretrained model')\n","            trained_model_weights = torch.load(pretrained_path)\n","            self.DistilledModel.load_state_dict(trained_model_weights['net'])\n","\n","        self.DistilledModel = self.DistilledModel.to(device)\n","        self.DistilledModel.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for batch_idx, (inputs, targets) in enumerate(self.testloader):\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                outputs = self.DistilledModel(inputs)\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","        test_acc = 100.*correct/total\n","\n","        print(f'Test Accuracy: {test_acc} ({correct}/{total})')\n","    \n","    def run(self, pretrained = False, pretrained_path = None, start_epoch = 0, num_epochs = 80):\n","\n","        if pretrained:\n","            print('loading pretrained model')\n","            trained_model_weights = torch.load(pretrained_path)\n","            self.DistilledModel.load_state_dict(trained_model_weights['net'])\n","\n","        self.DistilledModel = self.DistilledModel.to(device)\n","        self.EnsembleModel = self.EnsembleModel.to(device)\n","        \n","        with wandb.init(name=distillation_no_noise_variant, project='VOneNet_1'):\n","            for epoch in range(start_epoch, start_epoch+num_epochs):\n","                self.train(epoch)\n","\n","        print(self.best_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"oWFyJE3qU24U","outputId":"39c5018d-16d6-46c0-c103-8869ac627c95"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Training\n","save_dir = f'/content/drive/MyDrive/github/vonenet/checkpoint/DistillNoNoise/'\n","Model = TrainModel(model_dict, batch_size = 256, save_dir = save_dir)\n","Model.run(start_epoch = 1, num_epochs = 81)\n","\n","# Testing\n","Model.check_accuracy()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbaidyaavinash\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"output_type":"display_data","data":{"text/html":["\n","                Tracking run with wandb version 0.12.1<br/>\n","                Syncing run <strong style=\"color:#cdcd00\">conv_resnet</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://wandb.ai/baidyaavinash/VOneNet_1\" target=\"_blank\">https://wandb.ai/baidyaavinash/VOneNet_1</a><br/>\n","                Run page: <a href=\"https://wandb.ai/baidyaavinash/VOneNet_1/runs/2g4e4h58\" target=\"_blank\">https://wandb.ai/baidyaavinash/VOneNet_1/runs/2g4e4h58</a><br/>\n","                Run data is saved locally in <code>/content/vonenet/wandb/run-20210903_133610-2g4e4h58</code><br/><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 4.664870393245726 | Training Acc: 5.688 (5688/100000)\n","Validation Loss: 4.318284602104863 | Validation Acc: 9.39 (939/10000)\n","Saving..\n","\n","Epoch: 2\n","Training Loss: 3.899117456982508 | Training Acc: 14.895 (14895/100000)\n","Validation Loss: 4.1448576027833965 | Validation Acc: 12.97 (1297/10000)\n","Saving..\n","\n","Epoch: 3\n","Training Loss: 3.4901606512191656 | Training Acc: 21.436 (21436/100000)\n","Validation Loss: 3.4738124503365047 | Validation Acc: 21.27 (2127/10000)\n","Saving..\n","\n","Epoch: 4\n","Training Loss: 3.2668826921516674 | Training Acc: 25.364 (25364/100000)\n","Validation Loss: 3.4677123389666593 | Validation Acc: 22.94 (2294/10000)\n","Saving..\n","\n","Epoch: 5\n","Training Loss: 3.1041984183099265 | Training Acc: 28.219 (28219/100000)\n","Validation Loss: 3.188102260420594 | Validation Acc: 27.19 (2719/10000)\n","Saving..\n","\n","Epoch: 6\n","Training Loss: 2.9813883676553323 | Training Acc: 30.672 (30672/100000)\n","Validation Loss: 3.0959047818485694 | Validation Acc: 29.35 (2935/10000)\n","Saving..\n","\n","Epoch: 7\n","Training Loss: 2.8888375765222416 | Training Acc: 32.326 (32326/100000)\n","Validation Loss: 3.1356763296489474 | Validation Acc: 28.42 (2842/10000)\n","\n","Epoch: 8\n","Training Loss: 2.8092455766390048 | Training Acc: 33.726 (33726/100000)\n","Validation Loss: 3.02583845959434 | Validation Acc: 30.6 (3060/10000)\n","Saving..\n","\n","Epoch: 9\n","Training Loss: 2.7545030873144984 | Training Acc: 34.978 (34978/100000)\n","Validation Loss: 3.064918189109126 | Validation Acc: 29.5 (2950/10000)\n","\n","Epoch: 10\n","Training Loss: 2.7032957013000916 | Training Acc: 35.951 (35951/100000)\n","Validation Loss: 3.0082665727108338 | Validation Acc: 31.39 (3139/10000)\n","Saving..\n","\n","Epoch: 11\n","Training Loss: 2.668461491994541 | Training Acc: 36.723 (36723/100000)\n","Validation Loss: 3.068620066099529 | Validation Acc: 30.26 (3026/10000)\n","\n","Epoch: 12\n","Training Loss: 2.6403812481009443 | Training Acc: 37.192 (37192/100000)\n","Validation Loss: 3.044076970860928 | Validation Acc: 31.29 (3129/10000)\n","\n","Epoch: 13\n","Training Loss: 2.600122550564349 | Training Acc: 38.171 (38171/100000)\n","Validation Loss: 3.006824774078176 | Validation Acc: 31.52 (3152/10000)\n","Saving..\n","\n","Epoch: 14\n","Training Loss: 2.578465157152747 | Training Acc: 38.483 (38483/100000)\n","Validation Loss: 2.7412740490104577 | Validation Acc: 35.71 (3571/10000)\n","Saving..\n","\n","Epoch: 15\n","Training Loss: 2.5559176008414735 | Training Acc: 38.774 (38774/100000)\n","Validation Loss: 2.779673434511016 | Validation Acc: 34.77 (3477/10000)\n","\n","Epoch: 16\n","Training Loss: 2.538056313229339 | Training Acc: 39.085 (39085/100000)\n","Validation Loss: 2.9062557552434223 | Validation Acc: 32.39 (3239/10000)\n","\n","Epoch: 17\n","Training Loss: 2.5237326423835267 | Training Acc: 39.506 (39506/100000)\n","Validation Loss: 2.702971002723597 | Validation Acc: 36.3 (3630/10000)\n","Saving..\n","\n","Epoch: 18\n","Training Loss: 2.4988172560396706 | Training Acc: 40.073 (40073/100000)\n","Validation Loss: 2.8694321686708473 | Validation Acc: 33.83 (3383/10000)\n","\n","Epoch: 19\n","Training Loss: 2.4899005281650806 | Training Acc: 40.344 (40344/100000)\n","Validation Loss: 2.794575655007664 | Validation Acc: 35.11 (3511/10000)\n","\n","Epoch: 20\n","Training Loss: 2.479497766860611 | Training Acc: 40.465 (40465/100000)\n","Validation Loss: 2.6923274390305143 | Validation Acc: 36.74 (3674/10000)\n","Saving..\n","\n","Epoch: 21\n","Training Loss: 2.458485982302205 | Training Acc: 40.773 (40773/100000)\n","Validation Loss: 2.657489209235469 | Validation Acc: 37.52 (3752/10000)\n","Saving..\n","\n","Epoch: 22\n","Training Loss: 2.4545020717184256 | Training Acc: 40.976 (40976/100000)\n","Validation Loss: 2.7032816802399067 | Validation Acc: 37.05 (3705/10000)\n","\n","Epoch: 23\n","Training Loss: 2.4460184470466944 | Training Acc: 41.046 (41046/100000)\n","Validation Loss: 2.703456552722786 | Validation Acc: 36.83 (3683/10000)\n","\n","Epoch: 24\n","Training Loss: 2.442739138975168 | Training Acc: 41.238 (41238/100000)\n","Validation Loss: 2.7690725235999385 | Validation Acc: 36.29 (3629/10000)\n","\n","Epoch: 25\n","Training Loss: 2.43138279192283 | Training Acc: 41.361 (41361/100000)\n","Validation Loss: 2.7942154377321655 | Validation Acc: 35.06 (3506/10000)\n","\n","Epoch: 26\n","Training Loss: 2.425648731801211 | Training Acc: 41.563 (41563/100000)\n","Validation Loss: 2.6566086630277996 | Validation Acc: 38.11 (3811/10000)\n","Saving..\n","\n","Epoch: 27\n","Training Loss: 2.4218352255613906 | Training Acc: 41.621 (41621/100000)\n","Validation Loss: 2.7297586881661715 | Validation Acc: 36.27 (3627/10000)\n","\n","Epoch: 28\n","Training Loss: 2.4110637511438724 | Training Acc: 41.803 (41803/100000)\n","Validation Loss: 2.7997161285786687 | Validation Acc: 35.67 (3567/10000)\n","\n","Epoch: 29\n","Training Loss: 2.40844905147772 | Training Acc: 41.917 (41917/100000)\n","Validation Loss: 2.699004167242895 | Validation Acc: 37.58 (3758/10000)\n","\n","Epoch: 30\n","Training Loss: 2.3981868234436834 | Training Acc: 42.165 (42165/100000)\n","Validation Loss: 2.9503902513769606 | Validation Acc: 33.5 (3350/10000)\n","\n","Epoch: 31\n","Training Loss: 2.395678795664512 | Training Acc: 42.266 (42266/100000)\n","Validation Loss: 2.832702835903892 | Validation Acc: 35.01 (3501/10000)\n","\n","Epoch: 32\n","Training Loss: 2.3999373313716 | Training Acc: 42.096 (42096/100000)\n","Validation Loss: 2.7327130293544335 | Validation Acc: 35.91 (3591/10000)\n","\n","Epoch: 33\n","Training Loss: 1.7563161686863131 | Training Acc: 56.387 (56387/100000)\n","Validation Loss: 1.8779632049270822 | Validation Acc: 53.73 (5373/10000)\n","Saving..\n","\n","Epoch: 34\n","Training Loss: 1.543774157533865 | Training Acc: 60.82 (60820/100000)\n","Validation Loss: 1.813559206226204 | Validation Acc: 55.21 (5521/10000)\n","Saving..\n","\n","Epoch: 35\n","Training Loss: 1.445157108480668 | Training Acc: 62.915 (62915/100000)\n","Validation Loss: 1.7938909726806833 | Validation Acc: 55.57 (5557/10000)\n","Saving..\n","\n","Epoch: 36\n","Training Loss: 1.3710518082237 | Training Acc: 64.629 (64629/100000)\n","Validation Loss: 1.8245961575568477 | Validation Acc: 55.62 (5562/10000)\n","Saving..\n","\n","Epoch: 37\n","Training Loss: 1.3062299854310273 | Training Acc: 66.166 (66166/100000)\n","Validation Loss: 1.8003963609284992 | Validation Acc: 56.45 (5645/10000)\n","Saving..\n","\n","Epoch: 38\n","Training Loss: 1.2529368838843178 | Training Acc: 67.184 (67184/100000)\n","Validation Loss: 1.8368596004534372 | Validation Acc: 55.49 (5549/10000)\n","\n","Epoch: 39\n","Training Loss: 1.201354562778912 | Training Acc: 68.46 (68460/100000)\n","Validation Loss: 1.8185122239438793 | Validation Acc: 56.24 (5624/10000)\n","\n","Epoch: 40\n","Training Loss: 1.1597677826728967 | Training Acc: 69.445 (69445/100000)\n","Validation Loss: 1.8509965576703036 | Validation Acc: 56.01 (5601/10000)\n","\n","Epoch: 41\n","Training Loss: 1.117126686798642 | Training Acc: 70.374 (70374/100000)\n","Validation Loss: 1.8701035372818573 | Validation Acc: 55.68 (5568/10000)\n","\n","Epoch: 42\n","Training Loss: 0.8795547648464017 | Training Acc: 77.056 (77056/100000)\n","Validation Loss: 1.7385048051423664 | Validation Acc: 58.59 (5859/10000)\n","Saving..\n","\n","Epoch: 43\n","Training Loss: 0.8069008109362229 | Training Acc: 79.37 (79370/100000)\n","Validation Loss: 1.7237168414683282 | Validation Acc: 58.74 (5874/10000)\n","Saving..\n","\n","Epoch: 44\n","Training Loss: 0.7725742660901126 | Training Acc: 80.192 (80192/100000)\n","Validation Loss: 1.7343687286859826 | Validation Acc: 58.5 (5850/10000)\n","\n","Epoch: 45\n","Training Loss: 0.7459610051587414 | Training Acc: 80.887 (80887/100000)\n","Validation Loss: 1.728767912599105 | Validation Acc: 58.93 (5893/10000)\n","Saving..\n","\n","Epoch: 46\n","Training Loss: 0.7197799073613208 | Training Acc: 81.556 (81556/100000)\n","Validation Loss: 1.738689206823518 | Validation Acc: 58.34 (5834/10000)\n","\n","Epoch: 47\n","Training Loss: 0.6985563198699976 | Training Acc: 82.244 (82244/100000)\n","Validation Loss: 1.7426422804216795 | Validation Acc: 58.54 (5854/10000)\n","\n","Epoch: 48\n","Training Loss: 0.6767259583143932 | Training Acc: 82.724 (82724/100000)\n","Validation Loss: 1.7603447392017026 | Validation Acc: 58.27 (5827/10000)\n","\n","Epoch: 49\n","Training Loss: 0.6611922129112131 | Training Acc: 83.278 (83278/100000)\n","Validation Loss: 1.761366130430487 | Validation Acc: 58.59 (5859/10000)\n","\n","Epoch: 50\n","Training Loss: 0.6208356695483103 | Training Acc: 84.335 (84335/100000)\n","Validation Loss: 1.7503763105295882 | Validation Acc: 58.56 (5856/10000)\n","\n","Epoch: 51\n","Training Loss: 0.6157794346666092 | Training Acc: 84.535 (84535/100000)\n","Validation Loss: 1.7549349401570573 | Validation Acc: 58.58 (5858/10000)\n","\n","Epoch: 52\n","Training Loss: 0.6137634927735609 | Training Acc: 84.73 (84730/100000)\n","Validation Loss: 1.7498732880700993 | Validation Acc: 58.7 (5870/10000)\n","\n","Epoch: 53\n","Training Loss: 0.6123530672258123 | Training Acc: 84.649 (84649/100000)\n","Validation Loss: 1.7582929300356516 | Validation Acc: 58.46 (5846/10000)\n","\n","Epoch: 54\n","Training Loss: 0.6093734040513368 | Training Acc: 84.736 (84736/100000)\n","Validation Loss: 1.7543213608898693 | Validation Acc: 58.72 (5872/10000)\n","\n","Epoch: 55\n","Training Loss: 0.6063993848345773 | Training Acc: 84.921 (84921/100000)\n","Validation Loss: 1.7470230694058575 | Validation Acc: 58.69 (5869/10000)\n","\n","Epoch: 56\n","Training Loss: 0.6025239055800011 | Training Acc: 85.052 (85052/100000)\n","Validation Loss: 1.7431464255610598 | Validation Acc: 58.83 (5883/10000)\n","\n","Epoch: 57\n","Training Loss: 0.6062503655624512 | Training Acc: 84.891 (84891/100000)\n","Validation Loss: 1.7603199768669997 | Validation Acc: 58.5 (5850/10000)\n","\n","Epoch: 58\n","Training Loss: 0.6007194178336112 | Training Acc: 85.093 (85093/100000)\n","Validation Loss: 1.7540273183508763 | Validation Acc: 58.52 (5852/10000)\n","\n","Epoch: 59\n","Training Loss: 0.6014081176818179 | Training Acc: 85.062 (85062/100000)\n","Validation Loss: 1.7557020821148837 | Validation Acc: 58.46 (5846/10000)\n","\n","Epoch: 60\n","Training Loss: 0.6010594316913039 | Training Acc: 84.883 (84883/100000)\n","Validation Loss: 1.7489079946204076 | Validation Acc: 58.64 (5864/10000)\n","\n","Epoch: 61\n","Training Loss: 0.6032899318982268 | Training Acc: 85.032 (85032/100000)\n","Validation Loss: 1.7527058562145958 | Validation Acc: 58.44 (5844/10000)\n","\n","Epoch: 62\n","Training Loss: 0.601758191652615 | Training Acc: 85.034 (85034/100000)\n","Validation Loss: 1.7608038564271564 | Validation Acc: 58.35 (5835/10000)\n","\n","Epoch: 63\n","Training Loss: 0.5990119651150521 | Training Acc: 85.121 (85121/100000)\n","Validation Loss: 1.7496597389631634 | Validation Acc: 58.65 (5865/10000)\n","\n","Epoch: 64\n","Training Loss: 0.6032972324763417 | Training Acc: 85.051 (85051/100000)\n","Validation Loss: 1.7493347895296314 | Validation Acc: 58.8 (5880/10000)\n","\n","Epoch: 65\n","Training Loss: 0.5991042964065166 | Training Acc: 85.176 (85176/100000)\n","Validation Loss: 1.758082095580765 | Validation Acc: 58.35 (5835/10000)\n","\n","Epoch: 66\n","Training Loss: 0.6008556546914913 | Training Acc: 85.145 (85145/100000)\n","Validation Loss: 1.755939249750934 | Validation Acc: 58.37 (5837/10000)\n","\n","Epoch: 67\n","Training Loss: 0.6023553243226103 | Training Acc: 84.815 (84815/100000)\n","Validation Loss: 1.7607933297941956 | Validation Acc: 58.27 (5827/10000)\n","\n","Epoch: 68\n","Training Loss: 0.5992244640960718 | Training Acc: 85.132 (85132/100000)\n","Validation Loss: 1.7541860342025757 | Validation Acc: 58.76 (5876/10000)\n","\n","Epoch: 69\n","Training Loss: 0.6039735895898336 | Training Acc: 84.949 (84949/100000)\n","Validation Loss: 1.7451772659639768 | Validation Acc: 58.64 (5864/10000)\n","\n","Epoch: 70\n","Training Loss: 0.6002704280874004 | Training Acc: 85.164 (85164/100000)\n","Validation Loss: 1.7504195457772365 | Validation Acc: 58.59 (5859/10000)\n","\n","Epoch: 71\n","Training Loss: 0.5995984659780322 | Training Acc: 85.007 (85007/100000)\n","Validation Loss: 1.7499140485932556 | Validation Acc: 58.67 (5867/10000)\n","\n","Epoch: 72\n","Training Loss: 0.6032047267155269 | Training Acc: 85.026 (85026/100000)\n","Validation Loss: 1.7648737113687056 | Validation Acc: 58.28 (5828/10000)\n","\n","Epoch: 73\n","Training Loss: 0.6000856624539855 | Training Acc: 85.056 (85056/100000)\n","Validation Loss: 1.7515190583241136 | Validation Acc: 58.42 (5842/10000)\n","\n","Epoch: 74\n","Training Loss: 0.6010770352219071 | Training Acc: 85.101 (85101/100000)\n","Validation Loss: 1.7600269875948942 | Validation Acc: 58.62 (5862/10000)\n","\n","Epoch: 75\n","Training Loss: 0.6027875118853186 | Training Acc: 84.82 (84820/100000)\n","Validation Loss: 1.7538996150222006 | Validation Acc: 58.4 (5840/10000)\n","\n","Epoch: 76\n","Training Loss: 0.59826658086856 | Training Acc: 85.049 (85049/100000)\n","Validation Loss: 1.7575639999365504 | Validation Acc: 58.55 (5855/10000)\n","\n","Epoch: 77\n"]}]}]}